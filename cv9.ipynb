{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80ba4bb",
   "metadata": {},
   "source": [
    "1. What are the advantages of a CNN for image classification over a completely linked DNN?\n",
    "2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two,\n",
    "and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the\n",
    "top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does\n",
    "the CNN have in total? How much RAM would this network need when making a single instance\n",
    "prediction if we&#39;re using 32-bit floats? What if you were to practice on a batch of 50 images?\n",
    "3. What are five things you might do to fix the problem if your GPU runs out of memory while\n",
    "training a CNN?\n",
    "4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?\n",
    "5. When would a local response normalization layer be useful?\n",
    "6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and\n",
    "ResNet&#39;s core innovations?\n",
    "7. On MNIST, build your own CNN and strive to achieve the best possible accuracy.\n",
    "8. Using Inception v3 to classify broad images. a.\n",
    "Images of different animals can be downloaded. Load them in Python using the\n",
    "matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop\n",
    "them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency.\n",
    "The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to\n",
    "1.0, so make sure yours do as well.\n",
    "9. Large-scale image recognition using transfer learning.\n",
    "a. Make a training set of at least 100 images for each class. You might, for example, identify your\n",
    "own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as\n",
    "the flowers dataset or MIT&#39;s places dataset (requires registration, and it is huge).\n",
    "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also\n",
    "adding some randomness for data augmentation.\n",
    "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the\n",
    "last layer before output layer) and replace output layer with appropriate number of outputs for\n",
    "your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the\n",
    "output layer must have five neurons and use softmax activation function).\n",
    "d. Separate the data into two sets: a training and a test set. The training set is used to train the\n",
    "model, and the test set is used to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603e59f",
   "metadata": {},
   "source": [
    "# Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e80d897",
   "metadata": {},
   "source": [
    "1. CNNs have several advantages over completely connected DNNs for image classification. Firstly, CNNs are able to exploit the local structure of the input image, allowing them to extract more meaningful features. Secondly, CNNs are more computationally efficient because they use weight sharing and local connectivity to reduce the number of parameters. Finally, CNNs are more robust to translation and rotation because they can learn features that are invariant to these transformations.\n",
    "\n",
    "2. The CNN has a total of 1,980,900 parameters, calculated as follows:\n",
    "- First layer: (3 * 3 * 3) * 100 = 2700\n",
    "- Second layer: (3 * 3 * 100) * 200 = 180,000\n",
    "- Third layer: (3 * 3 * 200) * 400 = 1,152,000\n",
    "The RAM required for a single instance prediction using 32-bit floats would be approximately 36.8 MB (200 * 300 * 3 * 4 bytes). If practicing on a batch of 50 images, the RAM required would be approximately 1.84 GB (36.8 MB * 50).\n",
    "\n",
    "3. Five things you might do to fix the problem if your GPU runs out of memory while training a CNN are:\n",
    "- Reduce the batch size\n",
    "- Reduce the size of the input images\n",
    "- Use smaller or fewer layers in the model\n",
    "- Use a lower-precision data type (e.g. float16)\n",
    "- Use gradient checkpointing or other memory-efficient techniques\n",
    "\n",
    "4. Max pooling layers are useful in CNNs because they help to reduce the spatial dimensions of the feature maps while preserving the most important features. This allows the network to learn more robust and invariant features that are less sensitive to small translations and rotations.\n",
    "\n",
    "5. Local response normalization layers can be useful in CNNs when there are many overlapping filters or when there is a high level of correlation between adjacent feature maps. The normalization helps to increase the contrast between features and improve the generalization performance of the model.\n",
    "\n",
    "6. The main innovations in AlexNet include the use of ReLU activation functions, data augmentation techniques, dropout layers, and weight regularization. The main innovation in GoogLeNet is the use of the Inception module to extract features at multiple scales. The main innovation in ResNet is the use of skip connections to enable the training of very deep neural networks.\n",
    "\n",
    "7. Implementing a CNN on MNIST to achieve the best possible accuracy would involve several steps, including choosing an appropriate architecture, tuning hyperparameters such as learning rate and regularization strength, and implementing techniques like data augmentation and dropout. A possible architecture for this task could include several convolutional and pooling layers followed by several fully connected layers.\n",
    "\n",
    "8. Using Inception v3 to classify broad images would involve loading and preprocessing images, resizing and cropping them to 299 x 299 pixels, and scaling their pixel values to the range [-1, 1]. The pre-trained Inception v3 model can then be used as a feature extractor up to the bottleneck layer, and the output layer can be replaced with a new output layer for the desired classification task.\n",
    "\n",
    "9. Large-scale image recognition using transfer learning would involve creating a training set of at least 100 images for each class, preprocessing the images for data augmentation and resizing/cropping, and fine-tuning a pre-trained Inception v3 model for the desired classification task. The data can be separated into training and test sets, and the model can be evaluated on the test set to assess its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99390751",
   "metadata": {},
   "source": [
    "10.For this task of large-scale image recognition using transfer learning, the following steps can be followed:\n",
    "\n",
    "a. Make a training set of at least 100 images for each class. This can be done by identifying your own photos based on their position (beach, mountain, area, etc.) or by using an existing dataset such as the flowers dataset or MIT's places dataset. It is important to have a sufficient number of images for each class to ensure that the model can learn robust features.\n",
    "\n",
    "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also adding some randomness for data augmentation. This can be done using libraries such as TensorFlow or PyTorch. The images can be randomly cropped, rotated, and flipped to increase the diversity of the training data and prevent overfitting.\n",
    "\n",
    "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer and replace the output layer with an appropriate number of outputs for the new classification task. For example, if the dataset has five mutually exclusive classes, the output layer must have five neurons and use the softmax activation function. This process is called fine-tuning and allows the model to adapt the learned features to the new dataset.\n",
    "\n",
    "d. Separate the data into two sets: a training set and a test set. The training set is used to train the model, and the test set is used to evaluate its performance. The data should be split randomly to ensure that the model is tested on unseen data.\n",
    "\n",
    "By following these steps, it is possible to train a highly accurate image recognition model for a new classification task using transfer learning and a pre-trained Inception v3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397548b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
