{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e6aed6",
   "metadata": {},
   "source": [
    "1. What is the concept of cyclical momentum?\n",
    "2. What callback keeps track of hyperparameter values (along with other data) during\n",
    "training?\n",
    "3. In the color dim plot, what does one column of pixels represent?\n",
    "\n",
    "4. In color dim, what does &quot;poor teaching&quot; look like? What is the reason for this?\n",
    "\n",
    "5. Does a batch normalization layer have any trainable parameters?\n",
    "\n",
    "6. In batch normalization during preparation, what statistics are used to normalize? What\n",
    "about during the validation process?\n",
    "\n",
    "7. Why do batch normalization layers help models generalize better?\n",
    "\n",
    "8.Explain between MAX POOLING and AVERAGE POOLING is number eight.\n",
    "\n",
    "9. What is the purpose of the POOLING LAYER?\n",
    "\n",
    "10. Why do we end up with Completely CONNECTED LAYERS?\n",
    "\n",
    "11. What do you mean by PARAMETERS?\n",
    "\n",
    "12. What formulas are used to measure these PARAMETERS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851d6bf",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc558a35",
   "metadata": {},
   "source": [
    "1. Cyclical momentum is a technique used in stochastic gradient descent (SGD) optimization algorithms that involves varying the momentum coefficient over a cycle of iterations. This can help the optimizer to navigate high-curvature regions of the loss landscape more effectively and converge to a better solution.\n",
    "\n",
    "2. The Recorder callback keeps track of hyperparameter values, metrics, and other data during training. It can be used to generate visualizations, analyze performance, and save the results of the training process.\n",
    "\n",
    "3. In the color dim plot, one column of pixels represents the activation of a single filter in a convolutional layer. The rows represent the spatial locations of the activations across the input image.\n",
    "\n",
    "4. \"Poor teaching\" in the color dim plot would look like a lack of separation between the different classes or features in the activations. This could be due to a variety of reasons, such as a lack of diversity in the training data, suboptimal network architecture or hyperparameters, or insufficient training time.\n",
    "\n",
    "5. Yes, a batch normalization layer has trainable parameters, including the scale and shift parameters used to adjust the mean and variance of the activations during normalization.\n",
    "\n",
    "6. During training, batch normalization uses the mean and variance of the activations within a batch to normalize the data. During validation, the running mean and variance of the activations over the entire training set are used instead.\n",
    "\n",
    "7. Batch normalization layers help models generalize better by reducing the internal covariate shift, which is a change in the distribution of activations between layers. This can lead to more stable gradients, faster convergence, and better generalization performance.\n",
    "\n",
    "8. Max pooling and average pooling are both used in convolutional neural networks to downsample the feature maps and reduce the spatial dimensions of the data. Max pooling selects the maximum activation value within each pooling window, while average pooling takes the average of all the values. Max pooling is often used to preserve the most important features, while average pooling can be used to smooth out the data and reduce noise.\n",
    "\n",
    "9. The purpose of the pooling layer is to reduce the spatial dimensions of the feature maps and downsample the data. This can help to reduce the computational cost of the network, improve its ability to generalize, and prevent overfitting.\n",
    "\n",
    "10. We end up with completely connected layers (also known as dense layers) in order to perform the final classification or regression task. These layers take the output from the previous layers and apply a set of linear transformations to produce the final predictions.\n",
    "\n",
    "11. Parameters are the variables in a machine learning model that are learned during training. They include the weights and biases of the neurons in each layer, as well as any other learnable parameters used in the model, such as the scale and shift parameters in batch normalization.\n",
    "\n",
    "12. The formulas used to measure the parameters depend on the specific type of model and optimization algorithm being used. In general, the parameters are updated during training using a gradient-based optimization algorithm, such as stochastic gradient descent, and are measured using metrics such as the loss function, accuracy, or other evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146fc30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
