{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee54ad22",
   "metadata": {},
   "source": [
    "1. After each stride-2 conv, why do we double the number of filters?\n",
    "2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
    "3. What data is saved by ActivationStats for each layer?\n",
    "\n",
    "4. How do we get a learner&#39;s callback after they&#39;ve completed training?\n",
    "\n",
    "5. What are the drawbacks of activations above zero?\n",
    "\n",
    "6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "\n",
    "7. Why should we avoid starting training with a high learning rate?\n",
    "\n",
    "8. What are the pros of studying with a high rate of learning?\n",
    "\n",
    "9. Why do we want to end the training with a low learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734787b",
   "metadata": {},
   "source": [
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986f2d1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Doubling the number of filters after each stride-2 convolution allows the network to learn more complex features at higher resolutions in deeper layers. This is because the receptive field of each filter increases with each convolutional layer, so a greater number of filters are needed to capture a wider range of features.\n",
    "\n",
    "2. The larger kernel size in the first convolutional layer of a simple CNN for MNIST is useful because it allows the network to learn larger, more general features at the start of training. This can help the network to better understand the structure of the images and improve its ability to classify them.\n",
    "\n",
    "3. ActivationStats saves the mean, standard deviation, maximum, and minimum activation values for each layer during training. This information can be used to diagnose issues with the network, such as vanishing or exploding gradients, and to optimize the network's architecture or hyperparameters.\n",
    "\n",
    "4. To get a learner's callback after training, you can pass a callback function to the fit method of the learner. This function will be called after each epoch, and you can use it to perform any necessary post-training actions.\n",
    "\n",
    "5. The main drawback of activations above zero is that they can lead to vanishing or exploding gradients, which can make training unstable or prevent the network from converging to an optimal solution. Additionally, activations that are too high can lead to overfitting or numerical instability.\n",
    "\n",
    "6. The main benefit of practicing with larger batches is that it can lead to faster training times and better utilization of hardware resources, such as GPUs. However, larger batches can also lead to overfitting and lower accuracy, especially if the network is not large enough to handle the increased amount of data. Additionally, larger batches can require more memory and may be less efficient for online or real-time learning scenarios.\n",
    "\n",
    "7. Starting training with a high learning rate can cause the network's weights to update too quickly and overshoot the optimal solution. This can lead to instability, oscillations, or a suboptimal solution. It is generally recommended to start with a lower learning rate and gradually increase it as the network converges.\n",
    "\n",
    "8. The main advantage of training with a high learning rate is that it can lead to faster convergence and improved generalization, especially if the network is large and the dataset is complex. Additionally, high learning rates can help the network escape from local minima and explore a wider range of solutions.\n",
    "\n",
    "9. Ending training with a low learning rate can help the network to fine-tune its weights and converge to a more stable and accurate solution. This is because a low learning rate allows the network to make smaller and more precise weight updates, which can help it to avoid oscillations or overshooting the optimal solution. Additionally, a low learning rate can help to regularize the network and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086f282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
