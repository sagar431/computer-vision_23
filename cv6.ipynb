{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3834bdd8",
   "metadata": {},
   "source": [
    "1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n",
    "2. In the CNN architecture, where does the DROPOUT LAYER go?\n",
    "3. What is the optimal number of hidden layers to stack?\n",
    "\n",
    "4. In each layer, how many secret units or filters should there be?\n",
    "\n",
    "5. What should your initial learning rate be?\n",
    "\n",
    "6. What do you do with the activation function?\n",
    "\n",
    "7. What is NORMALIZATION OF DATA?\n",
    "\n",
    "8. What is IMAGE AUGMENTATION and how does it work?\n",
    "\n",
    "9. What is DECLINE IN LEARNING RATE?\n",
    "\n",
    "What does EARLY STOPPING CRITERIA mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ec563",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5689f9f",
   "metadata": {},
   "source": [
    "1. Trainable parameters are the parameters of a neural network that are updated during training, such as the weights and biases. Non-trainable parameters are fixed parameters that are not updated during training, such as the hyperparameters of the network or fixed pre-trained weights in a pre-trained network.\n",
    "\n",
    "2. The dropout layer is typically inserted after the convolutional and fully connected layers in a CNN architecture. The dropout layer randomly drops out (sets to zero) a specified fraction of the neurons in the previous layer during each training iteration, which can help prevent overfitting and improve generalization performance.\n",
    "\n",
    "3. The optimal number of hidden layers to stack depends on the complexity of the problem being solved, the size of the dataset, and the computational resources available. In general, it is recommended to start with a small number of layers and gradually increase the depth until the performance starts to plateau or degrade.\n",
    "\n",
    "4. The number of filters or neurons in each layer depends on the complexity of the problem being solved, the size of the dataset, and the computational resources available. It is generally recommended to start with a small number of filters or neurons and gradually increase the size until the performance starts to plateau or degrade.\n",
    "\n",
    "5. The initial learning rate should be set based on the optimizer being used and the specific problem being solved. In general, it is recommended to start with a small learning rate and gradually increase it during training to avoid overshooting the optimal solution.\n",
    "\n",
    "6. The activation function is used to introduce non-linearity into the network and is typically applied after each layer. The choice of activation function depends on the problem being solved and the architecture of the network.\n",
    "\n",
    "7. Normalization of data refers to the process of scaling the input features to a similar range to improve the convergence of the network and prevent numerical instability. Common techniques for normalization include standard scaling, min-max scaling, and z-score normalization.\n",
    "\n",
    "8. Image augmentation is a technique used to increase the size of the training dataset by generating new, modified versions of the existing images. This can include techniques like flipping, rotating, cropping, and scaling the images to increase the variability in the dataset and improve the generalization performance of the model.\n",
    "\n",
    "9. A decline in learning rate refers to the process of decreasing the learning rate during training to improve the convergence of the network and prevent overshooting the optimal solution. This can be done through various techniques like learning rate schedulers, which adjust the learning rate based on the number of epochs or the performance of the model.\n",
    "\n",
    "10. Early stopping criteria is a technique used to stop the training process early when the performance of the model starts to degrade or plateau. This is typically done by monitoring the validation loss or accuracy and stopping the training process when it no longer improves or starts to degrade. This can help prevent overfitting and improve the generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30686990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
